{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large technology firm needs your help, they've been hacked! \n",
    "Luckily their forensic engineers have grabbed valuable data about the hacks, \n",
    "including information like session time,locations, wpm typing speed, etc. \n",
    "The forensic engineer relates to you what she has been able to figure out so far, she has been able to \n",
    "grab meta data of each session that the hackers used to connect to their servers.\n",
    "The technology firm has 3 potential hackers that perpetrated the attack. \n",
    "Their certain of the first two hackers but they aren't very sure if the third hacker was involved or not. \n",
    "They have requested your help! Can you help figure out whether or not the third suspect had anything to do \n",
    "with the attacks, or was it just two hackers? It's probably not possible to know for sure, \n",
    "but maybe what you've just learned about Clustering can help!\n",
    "One last key fact, the forensic engineer knows that the hackers trade off attacks. \n",
    "Meaning they should each have roughly the same amount of attacks. For example if there were 100 total attacks, \n",
    "then in a 2 hacker situation each should have about 50 hacks, in a three hacker situation each would have \n",
    "about 33 hacks. The engineer believes this is the key element to solving this, \n",
    "but doesn't know how to distinguish this unlabeled data into groups of hackers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data descriotin:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'Session_Connection_Time': How long the session lasted in minutes\n",
    "'Bytes Transferred': Number of MB transferred during session\n",
    "'Kali_Trace_Used': Indicates if the hacker was using Kali Linux\n",
    "'Servers_Corrupted': Number of server corrupted during the attack\n",
    "'Pages_Corrupted': Number of pages illegally accessed\n",
    "'Location': Location attack came from (Probably useless because the hackers used VPNs)\n",
    "'WPM_Typing_Speed': Their estimated typing speed based on session logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creation of the spark session\n",
    "spark = SparkSession.builder.appName('hackers').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "dataset = spark.read.csv('hack_data.csv', header=True ,inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\n",
      "|Session_Connection_Time|Bytes Transferred|Kali_Trace_Used|Servers_Corrupted|Pages_Corrupted|            Location|WPM_Typing_Speed|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\n",
      "|                    8.0|           391.09|              1|             2.96|            7.0|            Slovenia|           72.37|\n",
      "|                   20.0|           720.99|              0|             3.04|            9.0|British Virgin Is...|           69.08|\n",
      "|                   31.0|           356.32|              1|             3.71|            8.0|             Tokelau|           70.58|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look to the data\n",
    "dataset.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Session_Connection_Time',\n",
       " 'Bytes Transferred',\n",
       " 'Kali_Trace_Used',\n",
       " 'Servers_Corrupted',\n",
       " 'Pages_Corrupted',\n",
       " 'Location',\n",
       " 'WPM_Typing_Speed']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking \n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instancing of the VectorAssembler\n",
    "# Here I choose which variable to use as features for the model\n",
    "assembler = VectorAssembler(inputCols= ['Session_Connection_Time','Bytes Transferred','Kali_Trace_Used',\n",
    "                                        'Servers_Corrupted','Pages_Corrupted','WPM_Typing_Speed'],\n",
    "                             outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transforming the dataset through the assembler object\n",
    "final_data = assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the K-means clustering model, in order to allow a good performance of the model itself we need to normalize the variables data. Link: https://en.wikipedia.org/wiki/Curse_of_dimensionality .In this case we use the modul StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import \n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instancing of the StandardScaler. \n",
    "# It takes in imput all the \"features\" and gives in output all the values normalized \n",
    "scaler = StandardScaler(inputCol='features', outputCol='scalaredFeatures', withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting of the data\n",
    "scaler_model = scaler.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transforming of the data\n",
    "finalData = scaler_model.transform(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step we our dataset normalized and ready to use for k-means clustering model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instancing of the KMeans method. \n",
    "# Here I pay attention to specify on wich column values the clusters must be calculated.\n",
    "# The aim of the job is to predict if there are 2 o 3 groups of hackers, so I calulate 3 and 3 clusters.\n",
    "\n",
    "k_means_2 = KMeans(featuresCol='scalaredFeatures', k=2) \n",
    "k_means_3 = KMeans(featuresCol='scalaredFeatures', k=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting of the normalized data\n",
    "kmeansModel_2 = k_means_2.fit(finalData)\n",
    "kmeansModel_3 = k_means_3.fit(finalData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's better to remember the following advise: one last key fact, the forensic engineer knows that the hackers trade off attacks. Meaning they should each have roughly the same amount of attacks. For example if there were 100 total attacks, then in a 2 hacker situation each should have about 50 hacks, in a three hacker situation each would have about 33 hacks. The engineer believes this is the key element to solving this, but doesn't know how to distinguish this unlabeled data into groups of hackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         2|   84|\n",
      "|         0|   83|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So, first i check how are the counts on three clusters\n",
    "kmeansModel_3.transform(finalData).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On three clusters we don't get what we should expect (3 * 33%) of the attacks. Let's try on two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         0|  167|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kmeansModel_2.transform(finalData).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Nice result! On two clusters we have the 50% and 50% of sharing attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#kmeansModel_2.transform(finalData).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can evaluate the performance of our model by the Within Set Sum of Squared Errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors K=2 = 601.7707512676716\n",
      "Within Set Sum of Squared Errors K=3 = 434.1492898715845\n"
     ]
    }
   ],
   "source": [
    "# Reminding: at the growing of K the Within Set Sum of Squared Errors goes down\n",
    "wssse_2 = kmeansModel_2.computeCost(finalData)\n",
    "wssse_3 = kmeansModel_3.computeCost(finalData)\n",
    "print(\"Within Set Sum of Squared Errors K=2 = \" + str(wssse_2))\n",
    "print(\"Within Set Sum of Squared Errors K=3 = \" + str(wssse_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have accomplished our task, our outputs say that the hacker attacks are divided in two groups and not in three. In this case we can confirm to the forensing engeneers what they have supposed. With our model they can distinct every attack!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last thing we can print out the centroid of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CENTROID CON K=2\n",
      "[ 2.99991988  2.92319035  1.05261534  3.20390443  4.51321315  3.28474   ]\n",
      "[ 1.26023837  1.31829808  0.99280765  1.36491885  2.5625043   5.26676612]\n",
      "\n",
      "\n",
      "CENTROID CON K=3\n",
      "[ 3.05623261  2.95754486  1.99757683  3.2079628   4.49941976  3.26738378]\n",
      "[ 1.26023837  1.31829808  0.99280765  1.36491885  2.5625043   5.26676612]\n",
      "[ 2.93719177  2.88492202  0.          3.19938371  4.52857793  3.30407351]\n"
     ]
    }
   ],
   "source": [
    "# Here I print the coordinates of the centroid\n",
    "centroid_2 = kmeansModel_2.clusterCenters()\n",
    "centroid_3 = kmeansModel_3.clusterCenters()\n",
    "print ('CENTROID CON K=2')\n",
    "for cent in centroidi_2:\n",
    "  print (cent)\n",
    "print ('\\n')\n",
    "print ('CENTROID CON K=3')\n",
    "for cent in centroidi_3:\n",
    "  print (cent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks for your attention!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "K-Means Cluster Project",
  "notebookId": 1165625175584032
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
